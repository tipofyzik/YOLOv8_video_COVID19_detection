{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889},{"sourceId":3174587,"sourceType":"datasetVersion","datasetId":717483},{"sourceId":9239986,"sourceType":"datasetVersion","datasetId":5589239},{"sourceId":9258955,"sourceType":"datasetVersion","datasetId":5571159}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing and importing all libraries for our project","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install ultralytics;\n%pip install -U ipywidgets\n%pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\n\nfrom ultralytics import YOLO\nimport xml.etree.ElementTree as ET\nfrom os import listdir, getcwd\nfrom os.path import join\nimport shutil\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SuppressOutput:\n    def __enter__(self):\n        self._stdout = sys.stdout\n        self._stderr = sys.stderr\n        sys.stdout = open(os.devnull, 'w')\n        sys.stderr = open(os.devnull, 'w')\n        \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout.close()\n        sys.stderr.close()\n        sys.stdout = self._stdout\n        sys.stderr = self._stderr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading and preparing data for training ML-models","metadata":{}},{"cell_type":"markdown","source":"### 1. Dataset for human detection","metadata":{}},{"cell_type":"code","source":"# Downloading dataset for human detection training\n!gdown 'https://drive.google.com/u/0/uc?id=1--0QuKMwj31K-CSvD8oq5fceFweiFPuN&export=download'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with SuppressOutput():\n    # Unpackaging\n    !unzip /kaggle/working/human_detection_dataset.zip\nprint(\"Unpacking complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/working/human_detection_dataset.zip\nprint(\"The origin zip-file has been deleted\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Dataset for mask detection","metadata":{}},{"cell_type":"code","source":"# Классы объектов (перечислите все классы, которые используются)\nclasses = ['without_mask', 'mask_weared_incorrect', 'with_mask']\n\n# Функция для конвертации координат\ndef convert_bbox_to_yolo(size, box):\n    if size[0] == 0:\n        dw = 1./(size[0]+0.00001)\n    else:\n        dw = 1./(size[0])\n        \n    if size[0] == 0:\n        dh = 1./(size[1]+0.00001)\n    else:\n        dh = 1./(size[1])\n    x_center = (box[0] + box[1]) / 2.0 - 1\n    y_center = (box[2] + box[3]) / 2.0 - 1\n    width = box[1] - box[0]\n    height = box[3] - box[2]\n    return (x_center * dw, y_center * dh, width * dw, height * dh)\n\n# Функция для конвертации XML-аннотации в YOLO-формат\ndef convert_xml_to_yolo(xml_file, output_dir):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n\n    # Получение размера изображения\n    size = root.find('size')\n    width = int(size.find('width').text)\n    height = int(size.find('height').text)\n\n    # Имя изображения\n    filename = root.find('filename').text\n    output_file = os.path.join(output_dir, os.path.splitext(filename)[0] + '.txt')\n\n    with open(output_file, 'w') as out_file:\n        for obj in root.iter('object'):\n            cls = obj.find('name').text\n            if cls not in classes:\n                continue\n            cls_id = classes.index(cls)\n            xmlbox = obj.find('bndbox')\n            bbox = (\n                float(xmlbox.find('xmin').text), \n                float(xmlbox.find('xmax').text),\n                float(xmlbox.find('ymin').text), \n                float(xmlbox.find('ymax').text)\n            )\n            bbox_yolo = convert_bbox_to_yolo((width, height), bbox)\n            out_file.write(f\"{cls_id} {' '.join(map(str, bbox_yolo))}\\n\")\n\n\n            \nxml_dir = '/kaggle/input/face-mask-detection/annotations'  # The original path to XML-files\nimage_dir = '/kaggle/input/face-mask-detection/images'  # The original path to XML-files\n\noutput_train_dir = '/kaggle/working/face-mask-detection/train'\noutput_val_dir = '/kaggle/working/face-mask-detection/val'\ntrain_ratio = 0.8  # Соотношение тренировочных данных\n\nos.makedirs(os.path.join(output_train_dir, 'images'), exist_ok=True)\nos.makedirs(os.path.join(output_train_dir, 'labels'), exist_ok=True)\nos.makedirs(os.path.join(output_val_dir, 'images'), exist_ok=True)\nos.makedirs(os.path.join(output_val_dir, 'labels'), exist_ok=True)\n\n# Получение всех XML файлов\nxml_files = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\nrandom.shuffle(xml_files)\n# Разделение на тренировочные и валидационные наборы\ntrain_size = int(len(xml_files) * train_ratio)\ntrain_files = xml_files[:train_size]\nval_files = xml_files[train_size:]\n\ndef move_files(xml_files, dest_dir):\n    for xml_file in xml_files:\n        image_file = xml_file.replace('.xml', '.png')\n        if not os.path.exists(os.path.join(image_dir, image_file)):\n            continue\n        convert_xml_to_yolo(os.path.join(xml_dir, xml_file), os.path.join(dest_dir, 'labels'))\n        shutil.copy(os.path.join(image_dir, image_file), os.path.join(dest_dir, 'images', image_file))\n\n# Перемещаем тренировочные и валидационные файлы\nmove_files(train_files, output_train_dir)\nmove_files(val_files, output_val_dir)\n\nprint(\"Convertation and split are completed.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trainig models (both are YOLOv8)","metadata":{}},{"cell_type":"code","source":"human_detection_model = YOLO('yolov8m.pt')\nmask_detection_model = YOLO('yolov8m.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_video_with_masks(model, video_path, output_path):\n    cap = cv2.VideoCapture(video_path)\n    \n    # Getting video parameters\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Creating VideoWriter object to write new video with predictions\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    class_colors = {\n        0: (0, 0, 255),\n        1: (0, 0, 255),\n        2: (0, 255, 0),\n    }\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Predicting\n        results = model(frame, iou = 0.4)\n\n        # Visualizing results        \n        for result in results:\n            for bbox in result.boxes:\n                class_id = int(bbox.cls)  # Ensure class_id is an integer\n                color = class_colors.get(class_id, (255, 255, 255))  # Default to white if class not in dictionary\n                \n                # Convert bbox.xyxy tensor to list\n                bbox_coords = bbox.xyxy.tolist()\n                x1, y1, x2, y2 = map(int, bbox_coords[0])\n                label = f'Class {class_id}'\n                \n                # Draw bounding box\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                # Draw label\n                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n        \n        # Writing annotated fram to new videofile\n        out.write(frame)\n    \n    # Resources release\n    cap.release()\n    out.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Training mask detection model","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_MODE'] = 'disabled'\n\nresult = mask_detection_model.train(\n    data='/kaggle/input/mask-data/mask_data.yaml',  # Path to data.yaml\n    epochs=35,  # Epoch number\n    imgsz=640,  # Image size\n    batch=16,    # Batch size\n)\nprint(\"Training of mask detection model complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_video_path = \"output_mask_video.avi\"\n# Применение модели к видео\nwith SuppressOutput():\n    process_video_with_masks(mask_detection_model, \"/kaggle/input/test-dataset/crowd_1280_720_30fps.mp4\", output_video_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Training human detection model","metadata":{}},{"cell_type":"code","source":"def process_video(model, video_path, output_path):\n    cap = cv2.VideoCapture(video_path)\n    \n    # Getting video parameters\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Creating VideoWriter object to write new video with predictions\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Predicting\n        results = model(frame, iou = 0.4, classes = [0])\n\n        # Visualizing results        \n        for result in results:\n            annotated_frame = result.plot()\n        \n        # Writing annotated fram to new videofile\n        out.write(annotated_frame)\n    \n    # Resources release\n    cap.release()\n    out.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_video_path = \"output_human_video.avi\"\n# Applying model to video\nwith SuppressOutput():\n    process_video(human_detection_model, \"/kaggle/input/test-dataset/crowd_1280_720_30fps.mp4\", output_video_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying both models","metadata":{}},{"cell_type":"code","source":"output_video_path = \"output_human_mask_video.avi\"\n# Applying model to video\nwith SuppressOutput():\n    process_video_with_masks(mask_detection_model, \"/kaggle/input/test-dataset/crowd_1280_720_30fps.mp4\", output_video_path)\nprint(\"Persons have beeen detected successfully\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_video_path = \"output_human_mask_video2.avi\"\n# Applying model to video\nwith SuppressOutput():\n    process_video(human_detection_model, \"/kaggle/working/output_human_mask_video.avi\", output_video_path)\nprint(\"Masks and persons have beeen detected successfully\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_people_and_masks_optimized(people_model, mask_model, video_path, output_path, iou_threshold=0.5):\n    cap = cv2.VideoCapture(video_path)\n    \n    # Getting video parameters\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Creating VideoWriter object to write new video with predictions\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Detect people\n        people_results = people_model(frame, iou=0.4)\n        people_boxes = people_results[0].boxes if len(people_results) > 0 else []\n\n        # Process each detected person\n        for person in people_boxes:\n            x1, y1, x2, y2 = map(int, person.xyxy[0].tolist())\n            \n            # Extract the region of interest (ROI) for mask detection\n            person_roi = frame[y1:y2, x1:x2]\n            \n            # Detect masks within the person's bounding box\n            mask_results = mask_model(person_roi, iou=0.4)\n            mask_boxes = mask_results[0].boxes if len(mask_results) > 0 else []\n            \n            # Draw bounding box for the person\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue bounding box for the person\n#             cv2.putText(frame, \"Person\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n            \n            # Draw bounding boxes for masks\n            for mask in mask_boxes:\n                mx1, my1, mx2, my2 = map(int, mask.xyxy[0].tolist())\n                \n                # Adjust mask bounding box coordinates to the original frame\n                mx1 += x1\n                my1 += y1\n                mx2 += x1\n                my2 += y1\n                \n                class_id = int(mask.cls[0])\n                \n                # Assign color based on mask class\n                if class_id == 0:  # Assume class 0 is \"proper mask\"\n                    color = (0, 0, 255)  # Red\n#                     label = \"No mask\"\n                elif class_id == 1:  # Assume class 1 is \"improper mask\"\n                    color = (0, 0, 255)  # Red\n#                     label = \"Improper Mask\"\n                else:  # Assume class 2 is \"no mask\"\n                    color = (0, 255, 0)  # Green\n#                     label = \"Mask\"\n                \n                cv2.rectangle(frame, (mx1, my1), (mx2, my2), color, 2)\n#                 cv2.putText(frame, label, (mx1, my1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n        \n        # Write the frame with annotations\n        out.write(frame)\n        \n    # Resources release\n    cap.release()\n    out.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_path = \"/kaggle/input/test-dataset/crowd_1280_720_30fps.mp4\"\nout_path = \"output_crowd_video.avi\"\nwith SuppressOutput():\n    detect_people_and_masks_optimized(human_detection_model, mask_detection_model, video_path, out_path)\nprint(\"Everithing have beeen detected successfully\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_path = \"/kaggle/input/test-dataset/crowd2_1280_720_30fps.mp4\"\nout_path = \"output_crowd_video2.avi\"\nwith SuppressOutput():\n    detect_people_and_masks_optimized(human_detection_model, mask_detection_model, video_path, out_path)\nprint(\"Everithing have beeen detected successfully\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"markdown","source":"### 1. Result for mask detection model","metadata":{}},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train'\nfiles = os.listdir(result_dir)\n\nif files:\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Image uploading via OpenCV\n        img = cv2.imread(img_path)\n\n        if img is not None:\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            plt.figure(figsize=(10, 10))\n            plt.imshow(img_rgb)\n            plt.axis('off')\n            plt.title(file)\n            plt.show()\n        else:\n            print(f\"Can't load the image: {file}\")\nelse:\n    print(\"Results haven't been saved. Check the path.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = mask_detection_model.predict(source='/kaggle/input/mask-data/test.jpeg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train2'\nfiles = os.listdir(result_dir)\n\nif files:\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Image uploading via OpenCV\n        img = cv2.imread(img_path)\n\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Отображение изображения\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img_rgb)\n        plt.axis('off')\n        plt.title(file)\n        plt.show()\nelse:\n    print(\"Результаты не были сохранены. Проверьте, правильно ли указан путь.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = mask_detection_model.predict(source='/kaggle/input/mask-data/masktypes.jpg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train3'\n# Список файлов в папке с результатами\nfiles = os.listdir(result_dir)\n\nif files:\n    # Вывод первых нескольких файлов для проверки\n    print(\"Файлы в папке с результатами:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Загрузка изображения с использованием OpenCV\n        img = cv2.imread(img_path)\n        # Преобразование изображения в формат RGB для отображения с Matplotlib\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Отображение изображения\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img_rgb)\n        plt.axis('off')\n        plt.title(file)\n        plt.show()\nelse:\n    print(\"Результаты не были сохранены. Проверьте, правильно ли указан путь.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Results for human detection model","metadata":{}},{"cell_type":"code","source":"# result_dir = '/kaggle/working/runs/detect/train2'\n# # Список файлов в папке с результатами\n# files = os.listdir(result_dir)\n\n# if files:\n#     print(\"Файлы в папке с результатами:\", files)\n\n#     for file in files:\n#         img_path = os.path.join(result_dir, file)\n#         # Загрузка изображения с использованием OpenCV\n#         img = cv2.imread(img_path)\n\n#         if img is not None:\n#             # Преобразование изображения в формат RGB для отображения с Matplotlib\n#             img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n#             # Отображение изображения\n#             plt.figure(figsize=(10, 10))\n#             plt.imshow(img_rgb)\n#             plt.axis('off')\n#             plt.title(file)\n#             plt.show()\n#         else:\n#             print(f\"Не удалось загрузить изображение: {file}\")\n# else:\n#     print(\"Результаты не были сохранены. Проверьте, правильно ли указан путь.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results = human_detection_model.predict(source='/kaggle/input/mask-data/test.jpeg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result_dir = '/kaggle/working/runs/detect/train22'\n# # Список файлов в папке с результатами\n# files = os.listdir(result_dir)\n\n# if files:\n#     # Вывод первых нескольких файлов для проверки\n#     print(\"Файлы в папке с результатами:\", files)\n\n#     for file in files:\n#         img_path = os.path.join(result_dir, file)\n#         # Загрузка изображения с использованием OpenCV\n#         img = cv2.imread(img_path)\n#         # Преобразование изображения в формат RGB для отображения с Matplotlib\n#         img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n#         # Отображение изображения\n#         plt.figure(figsize=(10, 10))\n#         plt.imshow(img_rgb)\n#         plt.axis('off')\n#         plt.title(file)\n#         plt.show()\n# else:\n#     print(\"Результаты не были сохранены. Проверьте, правильно ли указан путь.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results = human_detection_model.predict(source='/kaggle/input/mask-data/masktypes.jpg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result_dir = '/kaggle/working/runs/detect/train23'\n# # Список файлов в папке с результатами\n# files = os.listdir(result_dir)\n\n# if files:\n#     # Вывод первых нескольких файлов для проверки\n#     print(\"Файлы в папке с результатами:\", files)\n\n#     for file in files:\n#         img_path = os.path.join(result_dir, file)\n#         # Загрузка изображения с использованием OpenCV\n#         img = cv2.imread(img_path)\n#         # Преобразование изображения в формат RGB для отображения с Matplotlib\n#         img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n#         # Отображение изображения\n#         plt.figure(figsize=(10, 10))\n#         plt.imshow(img_rgb)\n#         plt.axis('off')\n#         plt.title(file)\n#         plt.show()\n# else:\n#     print(\"Результаты не были сохранены. Проверьте, правильно ли указан путь.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results = human_detection_model.predict(source='/kaggle/working/runs/detect/train4/masktypes.jpg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result_dir = '/kaggle/working/runs/detect/train24'\n# # Список файлов в папке с результатами\n# files = os.listdir(result_dir)\n\n# if files:\n#     # Вывод первых нескольких файлов для проверки\n#     print(\"Файлы в папке с результатами:\", files)\n\n#     for file in files:\n#         img_path = os.path.join(result_dir, file)\n#         # Загрузка изображения с использованием OpenCV\n#         img = cv2.imread(img_path)\n#         # Преобразование изображения в формат RGB для отображения с Matplotlib\n#         img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n#         # Отображение изображения\n#         plt.figure(figsize=(10, 10))\n#         plt.imshow(img_rgb)\n#         plt.axis('off')\n#         plt.title(file)\n#         plt.show()\n# else:\n#     print(\"Результаты не были сохранены. Проверьте, правильно ли указан путь.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}