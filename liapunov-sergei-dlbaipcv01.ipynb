{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889},{"sourceId":3174587,"sourceType":"datasetVersion","datasetId":717483},{"sourceId":9218846,"sourceType":"datasetVersion","datasetId":5571159},{"sourceId":9239986,"sourceType":"datasetVersion","datasetId":5589239}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing and importing all libraries for our project","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install ultralytics;\n%pip install -U ipywidgets\n%pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:38:18.509048Z","iopub.execute_input":"2024-08-25T05:38:18.509698Z","iopub.status.idle":"2024-08-25T05:38:59.837150Z","shell.execute_reply.started":"2024-08-25T05:38:18.509628Z","shell.execute_reply":"2024-08-25T05:38:59.836111Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\n\nfrom ultralytics import YOLO\nimport xml.etree.ElementTree as ET\nfrom os import listdir, getcwd\nfrom os.path import join\nimport shutil\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:38:59.839126Z","iopub.execute_input":"2024-08-25T05:38:59.839436Z","iopub.status.idle":"2024-08-25T05:39:03.631456Z","shell.execute_reply.started":"2024-08-25T05:38:59.839408Z","shell.execute_reply":"2024-08-25T05:39:03.630682Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class SuppressOutput:\n    def __enter__(self):\n        self._stdout = sys.stdout\n        self._stderr = sys.stderr\n        sys.stdout = open(os.devnull, 'w')\n        sys.stderr = open(os.devnull, 'w')\n        \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout.close()\n        sys.stderr.close()\n        sys.stdout = self._stdout\n        sys.stderr = self._stderr","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:39:03.632562Z","iopub.execute_input":"2024-08-25T05:39:03.632944Z","iopub.status.idle":"2024-08-25T05:39:03.639052Z","shell.execute_reply.started":"2024-08-25T05:39:03.632918Z","shell.execute_reply":"2024-08-25T05:39:03.637971Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Downloading and preparing data for training ML-models","metadata":{}},{"cell_type":"markdown","source":"### 1. Dataset for human detection","metadata":{}},{"cell_type":"code","source":"# Downloading dataset for human detection training\n!gdown 'https://drive.google.com/u/0/uc?id=1--0QuKMwj31K-CSvD8oq5fceFweiFPuN&export=download'","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:39:03.640291Z","iopub.execute_input":"2024-08-25T05:39:03.640622Z","iopub.status.idle":"2024-08-25T05:39:29.217770Z","shell.execute_reply.started":"2024-08-25T05:39:03.640590Z","shell.execute_reply":"2024-08-25T05:39:29.216719Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/u/0/uc?id=1--0QuKMwj31K-CSvD8oq5fceFweiFPuN&export=download\nFrom (redirected): https://drive.google.com/uc?id=1--0QuKMwj31K-CSvD8oq5fceFweiFPuN&export=download&confirm=t&uuid=b07e5c6c-15c0-4d8c-b46c-aa18e1549c97\nTo: /kaggle/working/human_detection_dataset.zip\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.67G/2.67G [00:20<00:00, 131MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"with SuppressOutput():\n    # Unpackaging\n    !unzip /kaggle/working/human_detection_dataset.zip\nprint(\"Unpacking complete\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:39:29.220588Z","iopub.execute_input":"2024-08-25T05:39:29.220950Z","iopub.status.idle":"2024-08-25T05:39:54.007704Z","shell.execute_reply.started":"2024-08-25T05:39:29.220920Z","shell.execute_reply":"2024-08-25T05:39:54.006546Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Unpacking complete\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm /kaggle/working/human_detection_dataset.zip\nprint(\"The origin zip-file has been deleted\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:39:54.009120Z","iopub.execute_input":"2024-08-25T05:39:54.009439Z","iopub.status.idle":"2024-08-25T05:39:55.403993Z","shell.execute_reply.started":"2024-08-25T05:39:54.009410Z","shell.execute_reply":"2024-08-25T05:39:55.402841Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The origin zip-file has been deleted\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. Dataset for mask detection","metadata":{}},{"cell_type":"code","source":"# ÐšÐ»Ð°ÑÑÑ‹ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² (Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»Ð¸Ñ‚Ðµ Ð²ÑÐµ ÐºÐ»Ð°ÑÑÑ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ)\nclasses = ['without_mask', 'mask_weared_incorrect', 'with_mask']\n\n# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚\ndef convert_bbox_to_yolo(size, box):\n    if size[0] == 0:\n        dw = 1./(size[0]+0.00001)\n    else:\n        dw = 1./(size[0])\n        \n    if size[0] == 0:\n        dh = 1./(size[1]+0.00001)\n    else:\n        dh = 1./(size[1])\n    x_center = (box[0] + box[1]) / 2.0 - 1\n    y_center = (box[2] + box[3]) / 2.0 - 1\n    width = box[1] - box[0]\n    height = box[3] - box[2]\n    return (x_center * dw, y_center * dh, width * dw, height * dh)\n\n# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸ XML-Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ð¸ Ð² YOLO-Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚\ndef convert_xml_to_yolo(xml_file, output_dir):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n\n    # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n    size = root.find('size')\n    width = int(size.find('width').text)\n    height = int(size.find('height').text)\n\n    # Ð˜Ð¼Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n    filename = root.find('filename').text\n    output_file = os.path.join(output_dir, os.path.splitext(filename)[0] + '.txt')\n\n    with open(output_file, 'w') as out_file:\n        for obj in root.iter('object'):\n            cls = obj.find('name').text\n            if cls not in classes:\n                continue\n            cls_id = classes.index(cls)\n            xmlbox = obj.find('bndbox')\n            bbox = (\n                float(xmlbox.find('xmin').text), \n                float(xmlbox.find('xmax').text),\n                float(xmlbox.find('ymin').text), \n                float(xmlbox.find('ymax').text)\n            )\n            bbox_yolo = convert_bbox_to_yolo((width, height), bbox)\n            out_file.write(f\"{cls_id} {' '.join(map(str, bbox_yolo))}\\n\")\n\n\n            \nxml_dir = '/kaggle/input/face-mask-detection/annotations'  # The original path to XML-files\nimage_dir = '/kaggle/input/face-mask-detection/images'  # The original path to XML-files\n\noutput_train_dir = '/kaggle/working/face-mask-detection/train'\noutput_val_dir = '/kaggle/working/face-mask-detection/val'\ntrain_ratio = 0.8  # Ð¡Ð¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…\n\nos.makedirs(os.path.join(output_train_dir, 'images'), exist_ok=True)\nos.makedirs(os.path.join(output_train_dir, 'labels'), exist_ok=True)\nos.makedirs(os.path.join(output_val_dir, 'images'), exist_ok=True)\nos.makedirs(os.path.join(output_val_dir, 'labels'), exist_ok=True)\n\n# ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… XML Ñ„Ð°Ð¹Ð»Ð¾Ð²\nxml_files = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\nrandom.shuffle(xml_files)\n# Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð½Ð°Ð±Ð¾Ñ€Ñ‹\ntrain_size = int(len(xml_files) * train_ratio)\ntrain_files = xml_files[:train_size]\nval_files = xml_files[train_size:]\n\ndef move_files(xml_files, dest_dir):\n    for xml_file in xml_files:\n        image_file = xml_file.replace('.xml', '.png')\n        if not os.path.exists(os.path.join(image_dir, image_file)):\n            continue\n        convert_xml_to_yolo(os.path.join(xml_dir, xml_file), os.path.join(dest_dir, 'labels'))\n        shutil.copy(os.path.join(image_dir, image_file), os.path.join(dest_dir, 'images', image_file))\n\n# ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÐ¼ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹\nmove_files(train_files, output_train_dir)\nmove_files(val_files, output_val_dir)\n\nprint(\"Convertation and split are completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:39:55.405635Z","iopub.execute_input":"2024-08-25T05:39:55.405994Z","iopub.status.idle":"2024-08-25T05:40:13.365989Z","shell.execute_reply.started":"2024-08-25T05:39:55.405964Z","shell.execute_reply":"2024-08-25T05:40:13.365066Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Convertation and split are completed.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Trainig models (both are YOLOv8)","metadata":{}},{"cell_type":"code","source":"human_detection_model = YOLO('yolov8m.pt')\nmask_detection_model = YOLO('yolov8m.pt')","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:40:13.367374Z","iopub.execute_input":"2024-08-25T05:40:13.367769Z","iopub.status.idle":"2024-08-25T05:40:14.438481Z","shell.execute_reply.started":"2024-08-25T05:40:13.367735Z","shell.execute_reply":"2024-08-25T05:40:14.437703Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m.pt to 'yolov8m.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.7M/49.7M [00:00<00:00, 234MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_video(model, video_path, output_path):\n    cap = cv2.VideoCapture(video_path)\n    \n    # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð²Ð¸Ð´ÐµÐ¾\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° VideoWriter Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸ÑÐ¼Ð¸\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹\n        results = model(frame)\n        \n        for result in results:\n            # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°\n            annotated_frame = result.plot()\n        \n        # Ð—Ð°Ð¿Ð¸ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð¾Ð³Ð¾ ÐºÐ°Ð´Ñ€Ð° Ð² Ð½Ð¾Ð²Ñ‹Ð¹ Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»\n        out.write(annotated_frame)\n    \n    # ÐžÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²\n    cap.release()\n    out.release()","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:40:14.439603Z","iopub.execute_input":"2024-08-25T05:40:14.439907Z","iopub.status.idle":"2024-08-25T05:40:14.447699Z","shell.execute_reply.started":"2024-08-25T05:40:14.439883Z","shell.execute_reply":"2024-08-25T05:40:14.446855Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 1. Training mask detection model","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_MODE'] = 'disabled'\n\nresult = mask_detection_model.train(\n    data='/kaggle/input/mask-data/mask_data.yaml',  # Path to data.yaml\n    epochs=35,  # ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¿Ð¾Ñ…\n    imgsz=640,  # Ð Ð°Ð·Ð¼ÐµÑ€ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n    batch=16,    # Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°\n)\nprint(\"Training of mask detection model complete\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:40:14.448979Z","iopub.execute_input":"2024-08-25T05:40:14.449317Z","iopub.status.idle":"2024-08-25T05:41:46.393549Z","shell.execute_reply.started":"2024-08-25T05:40:14.449287Z","shell.execute_reply":"2024-08-25T05:41:46.392399Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.81 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/kaggle/input/mask-data/mask_data.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 13.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=3\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n 22        [15, 18, 21]  1   3777433  ultralytics.nn.modules.head.Detect           [3, [192, 384, 576]]          \nModel summary: 295 layers, 25,858,057 parameters, 25,858,041 gradients, 79.1 GFLOPs\n\nTransferred 469/475 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 78.2MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/face-mask-detection/train/labels... 682 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 682/682 [00:01<00:00, 436.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/face-mask-detection/train/labels.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/face-mask-detection/val/labels... 171 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:00<00:00, 477.38it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/face-mask-detection/val/labels.cache\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 1 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"        1/1      7.16G      2.518      3.952      2.377        129        640:   2%|â–         | 1/43 [00:01<00:57,  1.38s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n        1/1      7.44G      1.623      1.908      1.392         55        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:24<00:00,  1.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.46s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all        171        744      0.417      0.543      0.484      0.285\n\n1 epochs completed in 0.013 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 52.0MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 52.0MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics YOLOv8.2.81 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 218 layers, 25,841,497 parameters, 0 gradients, 78.7 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all        171        744      0.415      0.543      0.484      0.285\n          without_mask         57        126      0.517      0.671      0.546      0.284\n mask_weared_incorrect         16         18     0.0544     0.0556     0.0287     0.0177\n             with_mask        152        600      0.673      0.903      0.878      0.553\nSpeed: 0.2ms preprocess, 7.8ms inference, 0.0ms loss, 3.7ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\nTraining of mask detection model complete\n","output_type":"stream"}]},{"cell_type":"code","source":"output_video_path = \"output_mask_video.avi\"\n# ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ðº Ð²Ð¸Ð´ÐµÐ¾\nprocess_video(mask_detection_model, \"/kaggle/input/test-dataset/crowd_1280_720_30fps.mp4\", output_video_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T05:41:46.395054Z","iopub.execute_input":"2024-08-25T05:41:46.395356Z","iopub.status.idle":"2024-08-25T05:41:53.971451Z","shell.execute_reply.started":"2024-08-25T05:41:46.395329Z","shell.execute_reply":"2024-08-25T05:41:53.970719Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\n0: 384x640 1 without_mask, 3 with_masks, 117.3ms\nSpeed: 1.8ms preprocess, 117.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 4 with_masks, 25.2ms\nSpeed: 1.7ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 25.1ms\nSpeed: 1.6ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 3 with_masks, 25.1ms\nSpeed: 1.6ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 2 with_masks, 24.8ms\nSpeed: 2.0ms preprocess, 24.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 3 with_masks, 24.7ms\nSpeed: 2.1ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 4 with_masks, 24.7ms\nSpeed: 1.6ms preprocess, 24.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 24.7ms\nSpeed: 1.6ms preprocess, 24.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 3 with_masks, 24.6ms\nSpeed: 1.5ms preprocess, 24.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 3 with_masks, 17.7ms\nSpeed: 1.7ms preprocess, 17.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 2 with_masks, 17.7ms\nSpeed: 1.6ms preprocess, 17.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 4 with_masks, 17.8ms\nSpeed: 1.9ms preprocess, 17.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 4 with_masks, 17.8ms\nSpeed: 1.5ms preprocess, 17.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 2 with_masks, 17.8ms\nSpeed: 2.1ms preprocess, 17.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 5 with_masks, 15.6ms\nSpeed: 1.9ms preprocess, 15.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 5 with_masks, 15.6ms\nSpeed: 2.1ms preprocess, 15.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 2 with_masks, 15.6ms\nSpeed: 1.5ms preprocess, 15.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 4 with_masks, 15.3ms\nSpeed: 1.7ms preprocess, 15.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 5 with_masks, 15.4ms\nSpeed: 2.0ms preprocess, 15.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 5 with_masks, 15.5ms\nSpeed: 1.7ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 13.7ms\nSpeed: 2.1ms preprocess, 13.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 3 with_masks, 13.8ms\nSpeed: 2.5ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 6 with_masks, 13.7ms\nSpeed: 1.8ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 4 with_masks, 13.5ms\nSpeed: 1.8ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 7 with_masks, 13.6ms\nSpeed: 1.9ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 13.6ms\nSpeed: 1.8ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 4 with_masks, 13.6ms\nSpeed: 1.9ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 4 with_masks, 13.5ms\nSpeed: 1.9ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 13.5ms\nSpeed: 1.6ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 4 with_masks, 13.4ms\nSpeed: 1.6ms preprocess, 13.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 4 with_masks, 13.4ms\nSpeed: 1.5ms preprocess, 13.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 6 with_masks, 13.5ms\nSpeed: 2.1ms preprocess, 13.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 3 with_masks, 13.0ms\nSpeed: 1.6ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 5 with_masks, 13.0ms\nSpeed: 1.6ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 5 with_masks, 12.9ms\nSpeed: 2.0ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 4 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 3 with_masks, 12.8ms\nSpeed: 1.5ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 3 with_masks, 12.7ms\nSpeed: 2.0ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 6 with_masks, 12.6ms\nSpeed: 1.7ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 6 with_masks, 12.7ms\nSpeed: 2.2ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 3 with_masks, 12.6ms\nSpeed: 1.9ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 3 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 12.5ms\nSpeed: 1.7ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 4 with_masks, 12.5ms\nSpeed: 2.0ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.7ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 7 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 4 with_masks, 12.5ms\nSpeed: 1.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 12.5ms\nSpeed: 2.0ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 6 with_masks, 12.6ms\nSpeed: 2.1ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 8 with_masks, 12.5ms\nSpeed: 1.7ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 7 with_masks, 12.5ms\nSpeed: 1.5ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 without_masks, 3 with_masks, 12.6ms\nSpeed: 2.0ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.5ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 12.6ms\nSpeed: 2.1ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 7 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 6 with_masks, 12.5ms\nSpeed: 1.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 7 with_masks, 12.5ms\nSpeed: 2.1ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 12.5ms\nSpeed: 1.9ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 8 with_masks, 12.5ms\nSpeed: 2.0ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 8 with_masks, 12.5ms\nSpeed: 1.9ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 12.5ms\nSpeed: 2.0ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 12.5ms\nSpeed: 1.5ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 without_masks, 10 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 11 with_masks, 12.6ms\nSpeed: 1.6ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 4 with_masks, 12.5ms\nSpeed: 1.7ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 4 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 9 with_masks, 12.5ms\nSpeed: 1.7ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 6 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 12.6ms\nSpeed: 2.0ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 5 with_masks, 12.5ms\nSpeed: 1.9ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 9 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 10 with_masks, 12.6ms\nSpeed: 1.9ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 12.5ms\nSpeed: 1.8ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 without_masks, 6 with_masks, 12.5ms\nSpeed: 1.5ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 7 with_masks, 12.5ms\nSpeed: 2.0ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 9 with_masks, 12.5ms\nSpeed: 1.8ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 8 with_masks, 12.6ms\nSpeed: 1.6ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 12.5ms\nSpeed: 1.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 3 with_masks, 12.5ms\nSpeed: 1.7ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 8 with_masks, 12.6ms\nSpeed: 2.1ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 8 with_masks, 12.7ms\nSpeed: 2.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 10 with_masks, 12.8ms\nSpeed: 1.8ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 12.8ms\nSpeed: 2.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 without_masks, 8 with_masks, 12.8ms\nSpeed: 2.2ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 12.8ms\nSpeed: 2.4ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 10 with_masks, 12.7ms\nSpeed: 2.1ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 8 with_masks, 12.7ms\nSpeed: 2.1ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 12.9ms\nSpeed: 2.0ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 12.9ms\nSpeed: 2.2ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 13 with_masks, 12.8ms\nSpeed: 1.7ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 without_masks, 10 with_masks, 12.8ms\nSpeed: 2.2ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 12 with_masks, 12.9ms\nSpeed: 2.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 without_masks, 10 with_masks, 12.8ms\nSpeed: 1.8ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 7 with_masks, 12.8ms\nSpeed: 2.1ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 7 with_masks, 12.8ms\nSpeed: 2.1ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 without_masks, 8 with_masks, 12.8ms\nSpeed: 2.1ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 12.8ms\nSpeed: 2.1ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 without_masks, 7 with_masks, 12.9ms\nSpeed: 2.2ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.1ms\nSpeed: 1.7ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 without_masks, 6 with_masks, 13.0ms\nSpeed: 1.9ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 13.0ms\nSpeed: 1.8ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 8 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 10 with_masks, 12.8ms\nSpeed: 2.0ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 13.2ms\nSpeed: 1.6ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 9 with_masks, 13.2ms\nSpeed: 1.6ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 13.3ms\nSpeed: 2.1ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 13.1ms\nSpeed: 1.9ms preprocess, 13.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 8 with_masks, 13.1ms\nSpeed: 2.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 7 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 13.0ms\nSpeed: 1.6ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 10 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 6 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 6 with_masks, 13.0ms\nSpeed: 1.6ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 8 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 5 with_masks, 13.0ms\nSpeed: 1.9ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 6 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 9 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 10 with_masks, 13.0ms\nSpeed: 1.6ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 with_masks, 13.3ms\nSpeed: 1.9ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 with_masks, 13.2ms\nSpeed: 1.4ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.3ms\nSpeed: 2.2ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 9 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 9 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.1ms\nSpeed: 2.0ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 13.2ms\nSpeed: 1.6ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 with_masks, 13.0ms\nSpeed: 1.5ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 6 with_masks, 13.1ms\nSpeed: 2.0ms preprocess, 13.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 13.1ms\nSpeed: 1.9ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 10 with_masks, 13.2ms\nSpeed: 1.6ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 7 with_masks, 13.2ms\nSpeed: 1.7ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.2ms\nSpeed: 2.1ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 9 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.0ms\nSpeed: 1.7ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 10 with_masks, 13.0ms\nSpeed: 2.2ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 with_masks, 12.9ms\nSpeed: 2.2ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 with_masks, 13.0ms\nSpeed: 2.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 11 with_masks, 13.0ms\nSpeed: 1.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 with_masks, 13.0ms\nSpeed: 1.8ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 9 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 with_masks, 13.0ms\nSpeed: 1.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 12.9ms\nSpeed: 1.7ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 8 with_masks, 12.9ms\nSpeed: 1.7ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 7 with_masks, 13.0ms\nSpeed: 1.7ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 7 with_masks, 12.9ms\nSpeed: 2.0ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 5 with_masks, 13.0ms\nSpeed: 1.7ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 without_masks, 8 with_masks, 13.0ms\nSpeed: 1.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 without_masks, 8 with_masks, 12.9ms\nSpeed: 2.1ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 8 with_masks, 12.9ms\nSpeed: 1.7ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 6 with_masks, 13.0ms\nSpeed: 1.6ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 with_masks, 13.0ms\nSpeed: 1.7ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 9 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 without_mask, 11 with_masks, 13.0ms\nSpeed: 2.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 with_masks, 13.0ms\nSpeed: 2.2ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 with_masks, 13.0ms\nSpeed: 1.7ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 with_masks, 13.0ms\nSpeed: 2.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 with_masks, 12.9ms\nSpeed: 2.2ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 with_masks, 13.0ms\nSpeed: 2.2ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 with_masks, 13.0ms\nSpeed: 2.0ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 with_masks, 12.9ms\nSpeed: 1.7ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 with_masks, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 12.9ms\nSpeed: 1.6ms preprocess, 12.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 with_masks, 13.2ms\nSpeed: 1.9ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 with_masks, 13.3ms\nSpeed: 2.0ms preprocess, 13.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 with_masks, 13.2ms\nSpeed: 1.6ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. Training human detection model","metadata":{}},{"cell_type":"code","source":"os.environ['WANDB_MODE'] = 'disabled'\n\nresult = human_detection_model.train(\n    data='/kaggle/working/human_detection_dataset/data.yaml',  # Path data.yaml\n    epochs=80,  # ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¿Ð¾Ñ…\n    imgsz=640,  # Ð Ð°Ð·Ð¼ÐµÑ€ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n    batch=16,    # Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_video_path = \"output_human_video.avi\"\n# ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ðº Ð²Ð¸Ð´ÐµÐ¾\nprocess_video(human_detection_model, \"/kaggle/input/test-dataset/crowd_1280_720_30fps.mp4\", output_video_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"markdown","source":"### 1. Result for mask detection model","metadata":{}},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train'\n# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸\nfiles = os.listdir(result_dir)\n\nif files:\n    print(\"Ð¤Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ OpenCV\n        img = cv2.imread(img_path)\n\n        if img is not None:\n            # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ RGB Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Matplotlib\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n            plt.figure(figsize=(10, 10))\n            plt.imshow(img_rgb)\n            plt.axis('off')\n            plt.title(file)\n            plt.show()\n        else:\n            print(f\"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ: {file}\")\nelse:\n    print(\"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ, Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = mask_detection_model.predict(source='/kaggle/input/mask-data/test.jpeg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train3'\n# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸\nfiles = os.listdir(result_dir)\n\nif files:\n    # Ð’Ñ‹Ð²Ð¾Ð´ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸\n    print(\"Ð¤Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ OpenCV\n        img = cv2.imread(img_path)\n        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ RGB Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Matplotlib\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img_rgb)\n        plt.axis('off')\n        plt.title(file)\n        plt.show()\nelse:\n    print(\"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ, Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = mask_detection_model.predict(source='/kaggle/input/mask-data/masktypes.jpg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train4'\n# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸\nfiles = os.listdir(result_dir)\n\nif files:\n    # Ð’Ñ‹Ð²Ð¾Ð´ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸\n    print(\"Ð¤Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ OpenCV\n        img = cv2.imread(img_path)\n        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ RGB Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Matplotlib\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img_rgb)\n        plt.axis('off')\n        plt.title(file)\n        plt.show()\nelse:\n    print(\"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ, Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Results for human detection model","metadata":{}},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train2'\n# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸\nfiles = os.listdir(result_dir)\n\nif files:\n    print(\"Ð¤Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ OpenCV\n        img = cv2.imread(img_path)\n\n        if img is not None:\n            # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ RGB Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Matplotlib\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n            plt.figure(figsize=(10, 10))\n            plt.imshow(img_rgb)\n            plt.axis('off')\n            plt.title(file)\n            plt.show()\n        else:\n            print(f\"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ: {file}\")\nelse:\n    print(\"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ, Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = human_detection_model.predict(source='/kaggle/input/mask-data/test.jpeg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train22'\n# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸\nfiles = os.listdir(result_dir)\n\nif files:\n    # Ð’Ñ‹Ð²Ð¾Ð´ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸\n    print(\"Ð¤Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ OpenCV\n        img = cv2.imread(img_path)\n        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ RGB Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Matplotlib\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img_rgb)\n        plt.axis('off')\n        plt.title(file)\n        plt.show()\nelse:\n    print(\"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ, Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = human_detection_model.predict(source='/kaggle/input/mask-data/masktypes.jpg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train23'\n# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸\nfiles = os.listdir(result_dir)\n\nif files:\n    # Ð’Ñ‹Ð²Ð¾Ð´ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸\n    print(\"Ð¤Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ OpenCV\n        img = cv2.imread(img_path)\n        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ RGB Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Matplotlib\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img_rgb)\n        plt.axis('off')\n        plt.title(file)\n        plt.show()\nelse:\n    print(\"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ, Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = human_detection_model.predict(source='/kaggle/working/runs/detect/train4/masktypes.jpg', save=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir = '/kaggle/working/runs/detect/train24'\n# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸\nfiles = os.listdir(result_dir)\n\nif files:\n    # Ð’Ñ‹Ð²Ð¾Ð´ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸\n    print(\"Ð¤Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸:\", files)\n\n    for file in files:\n        img_path = os.path.join(result_dir, file)\n        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ OpenCV\n        img = cv2.imread(img_path)\n        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ RGB Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Matplotlib\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img_rgb)\n        plt.axis('off')\n        plt.title(file)\n        plt.show()\nelse:\n    print(\"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ, Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}